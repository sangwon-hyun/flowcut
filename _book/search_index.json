[["index.html", "Creating the flowcut R package 1 Introduction", " Creating the flowcut R package Sheng Jiang, Sangwon Hyun 2025-02-15 1 Introduction This package implements flowcut, a Bayesian mixture of experts model used for censored data (specialized for ocean flow cytometry). The documentation and package are both created using one simple command: litr::render(&quot;index.Rmd&quot;, output_format = litr::litr_gitbook()) install.packages(&quot;~/repos/flowcut/flowcut&quot;, type=&#39;source&#39;,repos=NULL) ## install.packages(&quot;./flowcut/&quot;, repos = NULL, type=&quot;source&quot;) This line is purely for testing (to be deleted later!). litr::load_all(&quot;index.Rmd&quot;)##, output_format = litr::litr_gitbook()) my_load &lt;- function(){litr::render(&quot;~/repos/flowcut/index.Rmd&quot;, output_format = litr::litr_gitbook(minimal_eval = TRUE)) devtools::load_all(&quot;~/repos/flowcut/flowcut&quot;) } my_load() devtools::check(&quot;~/repos/flowcut/flowcut&quot;, document = FALSE) "],["package-setup.html", "2 Package setup", " 2 Package setup The DESCRIPTION file is created using this code. usethis::create_package( path = &quot;.&quot;, fields = list( Package = params$package_name, Version = &quot;0.0.0.9000&quot;, Title = &quot;flowcut&quot;, Description = &quot;Time-smooth mixture modeling for flow cytometry data.&quot;, `Authors@R` = person( given = &quot;Sangwon&quot;, family = &quot;Hyun&quot;, email = &quot;sangwonh@ucsc.edu&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;) ) ) ) usethis::use_mit_license(copyright_holder = &quot;Sangwon Hyun&quot;) The following is what will show up when someone types package?flowcut in the console. #&#39; flowcut #&#39; #&#39; This package implements the `flowcut` method. #&#39; #&#39; @docType package This package will have some dependancies: library(tidyverse) ##library(ggplot2) ##usethis::use_package(&quot;tidyverse&quot;, type = &quot;depends&quot;) ## usethis::use_package(&quot;ggplot2&quot;) usethis::use_package(&quot;ggplot2&quot;, type = &quot;depends&quot;) usethis::use_package(&quot;MASS&quot;, type = &quot;depends&quot;) usethis::use_package(&quot;parallel&quot;, type = &quot;depends&quot;) usethis::use_package(&quot;Rfast&quot;, type = &quot;depends&quot;) usethis::use_package(&quot;mvnfast&quot;, type = &quot;depends&quot;) usethis::use_package(&quot;stats&quot;, type = &quot;depends&quot;) usethis::use_package(&quot;matrixsampling&quot;, type = &quot;depends&quot;) usethis::use_package(&quot;matrixNormal&quot;, type = &quot;depends&quot;) usethis::use_package(&quot;pgdraw&quot;, type = &quot;depends&quot;) usethis::use_package(&quot;tmvnsim&quot;, type = &quot;depends&quot;) usethis::use_pipe(export=TRUE) "],["helpers-functions.html", "3 Helpers functions 3.1 Ball constraint on \\(X^{(t)} \\beta_k\\) (retire soon) 3.2 Closed-form \\(g\\) value in \\(\\beta\\)’s prior (\\(d=1\\)) 3.3 Closed-form \\(g\\) value in \\(\\beta\\)’s prior (\\(d=1\\)) 3.4 Other helpers", " 3 Helpers functions These are some helper functions that are needed for the MCMC. These functions are not documented for now. (TODO: do this). #&#39; A helper function to print the progress of a simulation. Place directly at #&#39; the beginning of the loop, before any computation happens. #&#39; @param isim isim. #&#39; @param nsim number of sim. #&#39; @param type type of job you&#39;re running. Defaults to &quot;simulation&quot;. #&#39; @param lapsetime lapsed time, in seconds (by default). #&#39; @param lapsetimeunit &quot;second&quot;. #&#39; @param start.time Start time, usually obtained using \\code{Sys.time()} #&#39; @param fill Whether or not to fill the line. #&#39; @param beep Whether to beep when done. printprogress &lt;- function(isim, nsim, type=&quot;simulation&quot;, lapsetime=NULL, lapsetimeunit=&quot;seconds&quot;, start.time=NULL, fill=FALSE, beep=FALSE){ ## If lapse time is present, then use it if(fill) cat(fill=TRUE) if(is.null(lapsetime) &amp; is.null(start.time)){ cat(&quot;\\r&quot;, type, &quot; &quot;, isim, &quot;out of&quot;, nsim) } else { if(!is.null(start.time)){ if(isim == 1){ lapsetime = 0 remainingtime = &quot;unknown&quot; endtime = &quot;unknown time&quot; } else { lapsetime = round(difftime(Sys.time(), start.time, units = &quot;secs&quot;), 0) remainingtime = round(lapsetime * (nsim - (isim - 1)) / (isim - 1), 0) endtime = strftime((Sys.time() + remainingtime)) } } cat(&quot;\\r&quot;, type, &quot; &quot;, isim, &quot;out of&quot;, nsim, &quot;with lapsed time&quot;, lapsetime, lapsetimeunit, &quot;and remaining time&quot;, remainingtime, lapsetimeunit, &quot;and will finish at&quot;, endtime, &quot;.&quot;) if(beep &amp; isim==nsim){beepr::beep()} } if(fill) cat(fill=TRUE) } #&#39; Helper function to print progress. #&#39; #&#39; @param iter Current iteration. #&#39; @param Nmc Total number of MCMC iterations. #&#39; #&#39; @export progress &lt;- function(iter, Nmc){ cat(&quot;MCMC iteration: &quot;, iter, &quot;.&quot;, round(iter/Nmc,3)*100, &quot;% is done&quot;, fill = TRUE) } wcrossprod.fast &lt;- function(x, w.sqrt, weighting = TRUE){ if(weighting){ x &lt;- x * w.sqrt } return(Rfast::Crossprod(x,x)) } #&#39; self.crossprod &lt;- function(x){ ## this function can be deleted. if(ncol(x) == 1){ ret &lt;- x %*% t(x) }else{ ret &lt;- crossprod(x) } return(ret) } #&#39; logsumexp &lt;- function (x) { y = max(x) y + log(sum(exp(x - y))) } #&#39; softmax &lt;- function (x) { exp(x - logsumexp(x)) } #&#39; softmax2 &lt;- function(x){ return(exp(x)/sum(exp(x))) } #&#39; logMNdensity &lt;- function(y,invSig,logdet){ ## The term &quot;-log(2*pi)*d/2&quot; is omitted return(- Rfast::Crossprod(y, invSig) %*% y /2 - logdet/2) } #&#39; KL.MN &lt;- function(p,q){ sum(p*log(p/q)) } #&#39; pgdraw.mod &lt;- function(b,c){ if(b&gt;0){ ret &lt;- pgdraw::pgdraw(b,c) }else{ ## print(&quot;in pgdraw: b=0&quot;) ret &lt;- 0 } return(ret) } #&#39; Helper to change stick-breaking vector |pi.sb| to regular probabilities. #&#39; @param pi.sb Stickbreaking probability vector. #&#39; #&#39; @return same sized vector of size K. #&#39; #&#39; @export SB2MN &lt;- function(pi.sb){ pi.sb &lt;- c(pi.sb,1) ## append 1 K &lt;- length(pi.sb) pi.mn &lt;- rep(0,K) pi.mn[1] &lt;- pi.sb[1] for(kk in 2:K){ pi.mn[kk] &lt;- pi.sb[kk]*(1-sum(pi.mn)) } return(pi.mn) } #&#39; Helper to change stick-breaking vector |pi.sb| to regular probabilities. The #&#39; opposite function of SB2MN. #&#39; @param pi.mn Probability vector. #&#39; #&#39; @return same sized vector of size K. #&#39; #&#39; @export MN2SB &lt;- function(pi.mn){ K &lt;- length(pi.mn) pi.sb &lt;- rep(0,K-1) pi.cs &lt;- cumsum(pi.mn[-K]) pi.sb[1] &lt;- pi.mn[1] pi.sb[2:(K-1)] &lt;- pi.mn[2:(K-1)]/(1-pi.cs[-(K-1)]) return(pi.sb) } #&#39; gamma2pi &lt;- function(gamma,Xp){ XpGamma &lt;- Rfast::Crossprod(gamma, Xp) ## K-1 x T pi.sb &lt;- 1/(1+exp(-XpGamma)) return(pi.sb) } #&#39; dtaMatCensor &lt;- function(x,Cbox){ d &lt;- dim(x)[2] for(ii in 1:d){ x[,ii] &lt;- pmin(pmax(x[,ii],Cbox[ii,1]),Cbox[ii,2]) } return(x) } #&#39; censorIndicator &lt;- function(x,Cbox){ d &lt;- dim(Cbox)[1] if(dim(x)[2]!=d &amp; dim(x)[1]==d){ x &lt;- t(x) } for(ii in 1:d){ x[,ii] &lt;- (x[,ii]==Cbox[ii,2]) - (x[,ii]==Cbox[ii,1]) } return(x) } #&#39; sample.region &lt;- function(cc,Cbox){ limits &lt;- NULL d &lt;- length(cc) for(i in 1:d){ if(cc[i]==-1){ limits &lt;- rbind(limits, c(-Inf,Cbox[i,1])) }else if(cc[i]==0){ limits &lt;- rbind(limits, c(-Inf,Inf)) }else{ limits &lt;- rbind(limits, c(Cbox[i,2], Inf)) } } colnames(limits) &lt;- c(&quot;lower&quot;,&quot;upper&quot;) return(limits) } #&#39; rmatnorm.fast &lt;- function(M,U,V){ ## M is d x p ## U is d x d ## V is p x p M &lt;- as.matrix(M) U &lt;- as.matrix(U) V &lt;- as.matrix(V) d &lt;- nrow(M) ## dim of U p &lt;- ncol(M) ## dim of V chol.U &lt;- Rfast::cholesky(U) ## upper triangular matrix chol.V &lt;- Rfast::cholesky(V) x &lt;- Rfast::matrnorm(d, p) return(Rfast::Crossprod(chol.U, x) %*% chol.V + M) } #&#39; samp.trunc.normal &lt;- function(yy, zz, cc.info, bounds.info, mu.mat, Sigma.ell, dimdat){ cc &lt;- abs(cc.info)==1 nc &lt;- cc==0 d &lt;- length(cc.info) lower.info &lt;- bounds.info[1:d] upper.info &lt;- bounds.info[(d+1):(2*d)] mu &lt;- mu.mat[,zz] ## If all dimensions are censored. if(all(cc)){ sig = Sigma.ell[,,zz] if(dimdat==1) sig = as.matrix(sig) yy &lt;- tmvnsim::tmvnsim(nsamp=1,d,lower=lower.info,upper=upper.info, mean= mu, sigma = sig)$samp ## If /not/ all dimensions are censored. } else { ## Conditional mean and variance slope &lt;- Sigma.ell[cc,nc,zz] %*% solve(Sigma.ell[nc,nc,zz]) cond.sig &lt;- Sigma.ell[cc,cc,zz] - slope %*% Sigma.ell[nc,cc,zz] cond.m &lt;- mu[cc]+slope %*% (yy[nc]-mu[nc]) ## Sample from this conditional truncated normal if(dimdat==1) cond.sig = as.matrix(cond.sig) stopifnot(&quot;matrix&quot; %in% class(cond.sig)) imputed_y = tmvnsim::tmvnsim(nsamp = 1, length(cond.m), lower=lower.info[cc], upper = upper.info[cc], mean= cond.m, sigma = cond.sig)$samp yy[cc] &lt;- imputed_y } if(!is.numeric(yy)) browser() return(yy) } #&#39; impute.censored &lt;- function(ww, yy, zz, cc.info.mat, bounds.mat, mu.mat, Sigma.ell, dimdat){ ## Setup stopifnot(&quot;array&quot; %in% class(Sigma.ell)) if(length(zz) == 0){ ret &lt;- NULL }else if (length(zz)==1){ ret &lt;- samp.trunc.normal(yy, zz, cc.info.mat, bounds.mat, mu.mat, Sigma.ell, dimdat) } else { ret &lt;- t(sapply(1:length(zz), function(ii) samp.trunc.normal(yy[ii,], zz[ii], cc.info.mat[ii,], bounds.mat[,ii], mu.mat, Sigma.ell, dimdat))) } return(ret) } #&#39; gen.syn.dta &lt;- function(T, K, p, d=3, Cbox=NULL, Pi =NULL, avg.clust.size=100){ ## this function can be deleted ### by default, d = 3 nt &lt;- stats::rpois(n=T,lambda=avg.clust.size*K) X &lt;- t(sapply(1:p, function(pp) stats::arima.sim(list(order=c(1,0,0), ar=pp/(pp+1)/2), n=T))) Beta &lt;- array(stats::rnorm(K*p*d), c(d,p,K)) beta0 &lt;- matrix(stats::rnorm(K*d), nrow=d,ncol=K) if(is.null(Pi)){ Gamma0 &lt;- matrix(stats::rnorm(K*p), nrow= p, ncol = K) Gamma0 &lt;- t(apply(Gamma0,1,function(x) x-x[K]))/10 gamma0 &lt;- (1:K)/1 Pi &lt;- t(apply(t(X) %*% Gamma0+gamma0, 1, function(x) exp(x)/sum(exp(x)))) } Sigma &lt;- stats::rWishart(K, 1, diag(d)) Z.list &lt;- lapply(1:T, function(tt) sample(1:K, nt[[tt]], prob = Pi[tt,],replace = TRUE)) Y.list &lt;- NULL for(t in 1:T){ Z &lt;- Z.list[[t]] Y.list[[t]] &lt;- t(sapply(Z,function(z) MASS::mvrnorm(n=1, mu = beta0[,z]+Beta[,,z]%*%X[,t],Sigma = Sigma[,,z]))) } Ytrue.list &lt;- Y.list if(is.null(Cbox)==FALSE){ Y.list &lt;- lapply(Y.list, function(x) dtaMatCensor(x,Cbox)) } return(list(Y.list=Y.list, X=X, Z.list=Z.list, nt=nt, Cbox=Cbox, K=K, Ytrue.list = Ytrue.list, Pi = Pi, beta0 = beta0, Beta = Beta, Sigma = Sigma)) } #&#39; loglik_eval &lt;- function(mu.list, chol.Sig.list, W.list, X.list, Y.list, Z.list, nt.list, simple = FALSE, n.cores = 1){ ## logPiZ &lt;- mcmapply(function(xx,yy,mm){sapply(1:K, function(kk) ## mvnfast::dmvn(yy, mm[,kk], chol.Sig.list[[kk]],log=TRUE,isChol = TRUE)) }, ## xx=X.list, yy = Y.list, mm=mu.list, mc.cores = min(n.cores, T), ## SIMPLIFY = FALSE) if(simple == TRUE){ ll.vec &lt;- sapply(1:nt.list[[1]], function(id) mvnfast::dmvn(Y.list[[1]][id,], mu.list[[1]][,Z.list[[1]][id]], chol.Sig.list[[Z.list[[1]][id]]], log=TRUE, isChol = TRUE)) out &lt;- sum(ll.vec * W.list[[1]]) / nt.list[[1]] }else{ ll.sums &lt;- parallel::mcmapply(function(ww, xx,yy,mm,zz,nt){ sum(ww * sapply(1:nt,function(id) mvnfast::dmvn(yy[id,], mm[,zz[id]], chol.Sig.list[[zz[id]]], log=TRUE,isChol = TRUE)) )}, ww = W.list, xx = X.list, yy = Y.list, mm=mu.list, zz = Z.list, nt = nt.list, mc.cores = min(n.cores, T), SIMPLIFY = TRUE) out &lt;- Reduce(&quot;+&quot;,ll.sums)/sum(unlist(nt.list)) } return(out) } Here’s a function that makes a censoring box (dimdat by 2 matrix, where the first column is the lower cutoff, and the second column is the upper cutoff). #&#39; Helper to obtain the censoring limits |Cbox|, which is a 2-colum matrix with columns |bounds.lower| and |bounds.upper|. #&#39; #&#39; @param ylist Data (list of d-column matrices) including censored particles. #&#39; #&#39; @return |dimdat| by 2 matrix. #&#39; @export get_Cbox &lt;- function(ylist){ ## Setup dimdat = ncol(ylist[[1]]) ## Define censor limits bounds.lower &lt;- Rfast::rowMins(matrix(unlist( lapply(ylist, function(xx) Rfast::colMins(xx,value = TRUE))), nrow = dimdat), value = TRUE) bounds.upper &lt;- Rfast::rowMaxs(matrix(unlist( lapply(ylist, function(xx) Rfast::colMaxs(xx,value = TRUE))), nrow = dimdat), value = TRUE) ## bounds.lower &lt;- lapply(ylist,function(xx) ## Rfast::colMins(xx,value = TRUE)) %&gt;% ## do.call(rbind,.) %&gt;% Rfast::colMins(.,value = TRUE) ## bounds.upper &lt;- lapply(ylist,function(xx) ## Rfast::colMaxs(xx,value = TRUE)) %&gt;% ## do.call(rbind,.) %&gt;% Rfast::colMaxs(.,value = TRUE) ## Modify censorship slightly Cbox &lt;- cbind(bounds.lower, bounds.upper) } 3.1 Ball constraint on \\(X^{(t)} \\beta_k\\) (retire soon) This function seems to have a bug. #&#39; Obtain the g parameter by simulation. #&#39; #&#39; @param X the design matrix, TT by p, (that doesn&#39;t include the intercept) #&#39; @param dimdat the dimension of the cytogram space #&#39; @param maxdev Maximum deviation of cluster means away from its grand mean. #&#39; @param numclust the number of experts #&#39; @param ggvec the vector of g parameter values to calculate the prior probability by Monte Carlo samples #&#39; @param Nmc Monte Carlo simulation sample size, with default value being 1e4. #&#39; @param n.cores the number of CPU cores to be used for parallelization #&#39; @param viz show the plot of the fitted relationship between the g parameter and the prior probability. #&#39; #&#39; #&#39; @return the g parameter with desired prior probability on maxdev #&#39; #&#39; @export maxdev_to_gg &lt;- function(X, dimdat, maxdev, numclust, ggvec, Nmc = 1e4 , prior.prob = 0.99, viz = FALSE, n.cores = 1, verbose = FALSE){ ## Basic setup p = ncol(X) ## Remember, X is T x p d = dimdat ## Helper function ## @param tX is the transpose of X ball.deviance &lt;- function(gg, rr, tX, Nmc=5000, nu0=d, nu1=p+1, S0=diag(d), S1=diag(p+1)){ inv.XTX &lt;- solve(tcrossprod(tX)) max.deviance &lt;- rep(NA,Nmc) Sig.ell &lt;- matrixsampling::rinvwishart(Nmc,nu0+d,S0) beta.ell &lt;- apply(Sig.ell,3,function(xx) as.matrix(matrixNormal::rmatnorm(M = matrix(0,d,p), U = xx, V = inv.XTX*gg, tol = .Machine$double.eps^0.5)), simplify = FALSE) xb &lt;- lapply(beta.ell, function(bb) bb%*% tX) # length of list: Nmc # each element: d x T max.deviance &lt;- lapply(xb, function(mat){ apply(mat,2,function(cols) crossprod(cols)) %&gt;% max() }) %&gt;% unlist() prob &lt;- mean(max.deviance &lt;= rr^2) return(list(gg = gg, rr = rr, prob = prob)) } gglist &lt;- as.list(ggvec)##as.list(1:40/100) n.cores &lt;- min(n.cores, length(gglist)) if(verbose) cat(&quot;Calculating gg value numerically&quot;, fill=TRUE) plist &lt;- parallel::mclapply(1:length(gglist), function(igg){ if(verbose) printprogress(igg, length(gglist), &quot;Candidate values&quot;) gg = gglist[[igg]] ball.deviance(gg, maxdev, t(X), Nmc = Nmc)$prob }, mc.cores = n.cores) if(verbose) cat(fill=TRUE) ## Make linear interpolation at a fine grid res = stats::approx(x = unlist(gglist), y = unlist(plist), method=&quot;linear&quot;, n = 100) newx = res$x newy = res$y ## Get closest point imin = which.min(abs(newy - prior.prob ^(1/numclust))) ## Make some plots to confirm if(viz){ plot(gglist,plist,type = &quot;l&quot;, ylab = &quot;Prob outside of \\n radius r&quot;, xlab = &quot;Value of |gg|&quot;) graphics::abline(h=0.05,lwd=2,col = &quot;red&quot;) graphics::abline(h=0.01,lwd=2,col = &quot;red&quot;) ## lines(y=newy, x=newx,lwd=2,col=&quot;blue&quot;) ## abline(v=newx[imin]) } return(newx[imin]) } 3.2 Closed-form \\(g\\) value in \\(\\beta\\)’s prior (\\(d=1\\)) We would like to choose a prior for \\(\\beta_k \\in \\mathbb{R}^{p \\times d}\\). Let’s first start with the simpler case of \\(d=1\\). Take a Normal prior: \\[ \\beta_k \\sim N(0, g V D^{-2} V^T) \\] where \\(V \\in \\mathbb{R}^{p \\times p}\\) is from the SVD \\(X = UDV^T\\). This way, we have a nice expression of the quantity we want to limit the size of: \\[w = X \\beta_k \\sim {N}(0, gUU^T) \\] Now, we want each entry of \\(w \\in \\mathbb{R}^T\\), \\(w_t \\in \\mathbb{R}\\), to be small in size; specifically, we can require that \\(1.96 \\cdot sd(w_t)\\) be smaller than \\(r\\), which ensures that \\(w_t\\) is usually smaller than \\(r\\): \\[P(w_t &lt; r) &gt; P(w_t &lt; 1.95 sd(w_t)) \\simeq 0.95.\\] In order to find the \\(g\\), we solve for \\(g\\) here: \\[1.96 \\cdot sd(w_t) = 1.96 \\sqrt{g} \\sqrt{(UU^T)_{t,t}} &lt; r\\] which works out to using a \\(g\\) that is smaller than: \\[ g &lt; \\frac{r^2}{1.96^2 (U U^T)_{t,t}}.\\] Now, if we want the size of \\(w_t\\) to be \\(typically\\) controlled at \\(0.95\\) across all \\(t\\)’s, we can have \\(g\\) valued at about: \\[ g \\simeq \\text{median}_{t=1\\cdots, T} \\frac{r^2}{1.96^2 (U U^T)_{t,t}}. \\] So we can choose to \\(g\\) to the value on the right hand side. This is implemented in maxdev_to_gg_closed_form_1d(). #&#39; Closed form for obtaining the g parameter by simulation. Currently only works for dimdat=1 #&#39; #&#39; @param X the design matrix, TT by p, (that doesn&#39;t include the intercept) #&#39; @param dimdat the dimension of the cytogram space #&#39; @param maxdev Maximum deviation of cluster means away from its grand mean. #&#39; @param prior_prob prior probability #&#39; #&#39; #&#39; @return the g parameter with desired prior probability on maxdev #&#39; #&#39; @export maxdev_to_g_closed_form_1d &lt;- function(X, dimdat, maxdev, prior_prob = 0.99){ stopifnot(dimdat == 1) obj = svd(X) pp = ncol(X) TT = nrow(X) ## maxdev = 0.115525 ## z_cutoff = 2.58 ## for 99% for one cluster ## z_cutoff = qnorm(1-(1-prior_prob^(1/numclust))/2) z_cutoff = qnorm(1-(1-prior_prob)/2) ## Form the constant c and matrix P, multiplied to form the prior covariance cP. P = obj$u %*% t(obj$u) c = (maxdev^2 / z_cutoff^2 / diag(P)) %&gt;% median A = c * obj$v %*% diag(1/(obj$d^2)) %*% t(obj$v) cP = (X %*% A %*% t(X)) ##%&gt;% diag() %&gt;% plot() return(c) } Let’s test out this function, numerically. We indeed see the correct coverage: set.seed(100) X = rnorm(1000,0,1) %&gt;% matrix(ncol = 10, nrow = 100) obj = svd(X) pp = ncol(X) TT = nrow(X) maxdev = 0.5 numclust = 1 prior_probs = seq(from = 0.1, to = 1, by = 0.05) sapply(prior_probs, function(prior_prob){ ## z_cutoff = qnorm(1-(1-prior_prob^(1/numclust))/2) z_cutoff = qnorm(1-(1-prior_prob)/2) ## Form the constant c and matrix P, multiplied to form the prior covariance cP. P = obj$u %*% t(obj$u) c = (maxdev^2 / z_cutoff^2 / diag(P)) %&gt;% median() ## c = (maxdev^2 / z_cutoff^2 / mean(diag(P))) A = c * obj$v %*% diag(1/(obj$d^2)) %*% t(obj$v) cP = (X %*% A %*% t(X)) ##%&gt;% diag() %&gt;% plot() ## Generate beta and form w = X beta_k many_betas = MASS::mvrnorm(100000, mu = rep(0, pp), Sigma = A) w = X %*% t(many_betas) ## Plotting if(FALSE){ ## Make a plot of all w = X beta_k w %&gt;% .[,1:500] %&gt;% matplot(pch = 16, col = rgb(0,0,0,0.1)) ## Draw the lines where the 95&#39;th quantiles are. qs = w %&gt;% apply(1, function(one_w){ quantile(one_w, probs=0.975)}) lines(qs, col = &#39;red&#39;) qs = w %&gt;% apply(1, function(one_w){ quantile(one_w, probs=0.025)}) lines(qs, col = &#39;red&#39;) abline(h = maxdev, lwd = 2, lty = 2, col = &#39;red&#39;) abline(h = -maxdev, lwd = 2, lty = 2, col = &#39;red&#39;) text(x=50, y = maxdev*1.1, labels = &quot;Maximum deviation&quot;) legend(&quot;topleft&quot;, col = &#39;red&#39;, lwd=1, lty=1, legend = c(&quot;0.95 quantile&quot;)) } sum((abs(w)&lt;maxdev))/length(w) }) -&gt; coverages plot(x = prior_probs, y = coverages, type = &#39;o&#39;, ylim = c(0,1), xlim = c(0,1), xlab = &quot;Target prior probability of deviation\\n of |w| = |X beta| above maxdev&quot;, ylab = &quot;Empirical probability of deviation&quot;) abline(0,1, col = &#39;red&#39;) 3.3 Closed-form \\(g\\) value in \\(\\beta\\)’s prior (\\(d=1\\)) Next, when \\(\\beta_k \\in \\mathbb{R}^{p \\times d}\\), we use a matrix normal prior \\[\\beta_k \\sim MN(0, g V D^{-2} V^T, I_d),\\] from which we see that: \\[w = X\\beta_k \\sim N(0, g UU^T, I_d).\\] Let’s quickly verify this in a QQ plot, so we know we aren’t crazy. Specifically, we verify that our calculation of the distribution of \\(w_t\\),: \\[w_t = w \\cdot e_t^T \\in \\mathbb{R}^{d},\\] which should be Normal with equal variance \\(g(UU^T)_{t,t}\\) in all \\(d=3\\) dimensions, is correct. library(matrixNormal) ## ## Attaching package: &#39;matrixNormal&#39; ## The following object is masked from &#39;package:base&#39;: ## ## I set.seed(112) X = rnorm(1000,0,1) %&gt;% matrix(ncol = 10, nrow = 100) obj = svd(X) pp = ncol(X) TT = nrow(X) dimdat = 3 P = obj$u %*% t(obj$u) maxdev = 0.5 c = (maxdev^2) / qchisq(0.95, 3) / max(diag(P)) nsim = 1000 ## Draw beta matrices set.seed(100) beta_draws &lt;- lapply(1:nsim, function(isim){ one_beta = matrixNormal::rmatnorm(M = matrix(0, ncol = dimdat, nrow = pp), U = c * obj$v %*% diag(1/(obj$d^2)) %*% t(obj$v), V = diag(1,dimdat)) }) ## Form w = X beta_k wlist = lapply(1:nsim, function(isim){ one_beta = beta_draws[[isim]] w = X %*% one_beta return(w) }) ## Verify that the theoretical variance of w = Xt * beta_k is diagonal and ## equal to c * U * U^T theoretical_variance = c * obj$u %*% t(obj$u) all_theoretical_diags = theoretical_variance %&gt;% diag() all_diags = sapply(1:TT, function(tt){ sapply(wlist, function(w){ w[tt,]}) %&gt;% t() %&gt;% var() %&gt;% diag() %&gt;% mean() }) plot(x=all_theoretical_diags, y=all_diags, main = &quot;Simulated vs. theoretical variances of\\nof X^{(t)} * beta_k&quot;, ylab = &quot;Simulated&quot;, xlab = &quot;Theoretical&quot;) abline(0,1) We are interested in the marginal variance of \\(t\\)’th row of \\(w\\): \\[w_t = w \\cdot e_t^T \\in \\mathbb{R}^{d}.\\] whose distribution is: \\[w_t \\sim N(0, \\underbrace{g (UU^T)_{t,t} \\cdot I_d}_{\\Omega})\\] Next, we want to find a constant \\(g\\) so that the size \\(\\|w_t\\|_2\\) of \\(w_t\\) is smaller than the radius \\(r = 0.5\\) with a high probability: \\[P(\\sqrt{w_t^T w_t} \\le r) &gt; 0.95.\\] The distribution of \\(w_t^T \\Omega^{-1} w_t = \\frac{1}{g (UU^T)_{t,t} } \\cdot w_t^T w_t\\) is distributed as a Chi-squared random variable with \\(d\\) degrees of freedom – \\(\\chi^2_d\\) – so that \\[ P( \\frac{1}{g (UU^T)_{t,t} } \\cdot w_t^T w_t \\le \\chi_{d, 0.95}^{2, *}) \\ge 0.95.\\] Rearranging, we get: \\[ P( w_t^T w_t \\le \\frac{1}{g (UU^T)_{t,t} } \\cdot \\chi_{d, 0.95}^{2, *}) \\ge 0.95.\\] whose upper bound we’d like to be equal to or smaller than \\(r^2 = 0.5^2\\): \\[\\frac{1}{g (UU^T)_{t,t} } \\cdot \\chi_{d, 0.95}^{2, *} = g (UU^T)_{t,t} \\cdot \\chi_{d, 0.95}^{2, *} \\le r^2.\\] From this, we see that \\(g\\) should be at most \\[g &lt; \\frac{r^2}{ (UU^T)_{t,t} \\chi_{d, 0.95}^{2, *}},\\] for a given time point \\(t\\). And like before, we will actually use \\(g = \\text{Median}( \\{\\frac{r^2}{ \\max_t{(UU^T)_{t,t}} \\chi_{d, 0.95}^{2, *}}, t=1,\\cdots, T\\} )\\) to ensure a typical \\(95\\%\\) control at any time \\(t\\). This is implemented in maxdev_to_g_closed_form_3d(). #&#39; Closed form for obtaining the g parameter by simulation. Currently only works for *any* dimdat. #&#39; #&#39; @param X the design matrix, TT by p, (that doesn&#39;t include the intercept) #&#39; @param dimdat the dimension of the cytogram space #&#39; @param maxdev Maximum deviation of cluster means away from its grand mean. #&#39; @param prior_prob prior probability #&#39; @param viz show the plot of the fitted relationship between the g parameter and the prior probability. #&#39; #&#39; @return the g parameter with desired prior probability on maxdev #&#39; #&#39; @export maxdev_to_g_closed_form_3d &lt;- function(X, dimdat, maxdev, prior_prob = 0.99, viz = FALSE){ ## stopifnot(dimdat == 3) obj = svd(X) pp = ncol(X) TT = nrow(X) ## maxdev = 0.115525 ## chisq_cutoff = qchisq(p=1-(1-prior_prob^(1/numclust)), df = dimdat) chisq_cutoff = qchisq(p=1-(1-prior_prob), df = dimdat) ## Form the constant c and matrix P, multiplied to form the prior covariance cP. P = obj$u %*% t(obj$u) ## gg = (maxdev / chisq_cutoff / diag(P)) %&gt;% median() gg = (maxdev^2 / chisq_cutoff / diag(P)) %&gt;% median() A = gg * obj$v %*% diag(1/(obj$d^2)) %*% t(obj$v) gP = (X %*% A %*% t(X)) ## prior covariance; not used now. if(viz){ ## Generate beta and form w = X beta_k nsim = 1000 many_betas = matrixsampling::rmatrixnormal(n = nsim, M=matrix(0, pp, dimdat), U=A, V = diag(rep(1,dimdat))) wlist = lapply(1:nsim, function(ii)X %*% many_betas[,,ii]) wlist_rownorms = wlist %&gt;% lapply(function(w) apply(w, 1, function(a)sqrt(sum(a*a)))) ## Make a plot of all |w|_2, w = X beta_k wlist_rownorms %&gt;% do.call(rbind, .) %&gt;% t() %&gt;% matplot(pch = 16, col = rgb(0,0,0,0.1), cex = .5, ylab = &quot;2norm of w[t]&quot;, xlab = &quot;t&quot;) ## Draw the lines where the 95&#39;th quantiles are. qs = wlist_rownorms %&gt;% do.call(rbind, .) %&gt;% t() %&gt;% apply(1, function(one_w){ quantile(one_w, probs=prior_prob)}) lines(qs, col = &#39;red&#39;) abline(h = maxdev, lwd = 2, lty = 2, col = &#39;red&#39;) abline(h = median(qs), lwd = 2, lty = 2, col = &#39;yellow&#39;) legend(&quot;topleft&quot;, col = &#39;red&#39;, lwd=1, lty=1, legend = c(paste0(prior_prob,&quot; quantile at each t&quot;)), bty = &quot;n&quot;) return(NULL) } else { return(gg) } } This should produce the same value \\(g\\) as the 1d case maxdev_to_g_closed_form_1d() if dimdat=1. maxdev_to_g_closed_form_3d(X, 1, maxdev, prior_prob = 0.99) ## [1] 0.4018007 maxdev_to_g_closed_form_1d(X, 1, maxdev, prior_prob = 0.99) ## [1] 0.4018007 The next plot shows a simulation; from a synthetic \\(X\\), we draw 1000 values of \\(\\beta\\) form the prior, then over all times \\(t=1,\\cdots, 100\\), (every vertical slice showing the simulated \\(w_t = X^{(t)} \\beta\\) values), we show (1) the 99% connected by a thin red line, (2) the desired typical deviation of these quantiles marked by a horizontal dashed red line, and (3) the actual median of these quantiles marked by a horizontal yellow line. set.seed(100) X = rnorm(1000,0,1) %&gt;% matrix(ncol = 10, nrow = 100) maxdev_to_g_closed_form_3d(X, 1, maxdev, prior_prob = 0.99, viz = TRUE) ## NULL It seems that the median is an appropriate choice. Here is some code to verify the coverage of the resulting parameter \\(g\\). For dimdat=3 (i.e., \\(d=3\\)), the control is mostly correct but with a small systematic error. set.seed(100) X = rnorm(1000,0,1) %&gt;% matrix(ncol = 10, nrow = 100) obj = svd(X) pp = ncol(X) TT = nrow(X) maxdev = 0.5 dimdat = 3 prior_probs = seq(from = 0.1, to = 1, by = 0.1) sapply(prior_probs, function(prior_prob){ chisq_cutoff = qchisq(p = (1-(1-prior_prob)), df = dimdat) ## chisq_cutoff = qchisq(p = (1-(1-prior_prob^(1/numclust))), df = dimdat) ## Form the constant c and matrix P, multiplied to form the prior covariance cP. P = obj$u %*% t(obj$u) c = (maxdev^2 / chisq_cutoff / diag(P)) %&gt;% median() A = c * obj$v %*% diag(1/(obj$d^2)) %*% t(obj$v) cP = (X %*% A %*% t(X)) ##%&gt;% diag() %&gt;% plot() ## Generate beta and form w = X beta_k nsim = 10000 many_betas = matrixsampling::rmatrixnormal(n = nsim, M = matrix(0, pp, dimdat), U = A, V = diag(rep(1,dimdat))) wlist = lapply(1:nsim, function(ii)X %*% many_betas[,,ii]) wlist_rownorms = wlist %&gt;% lapply(function(w) apply(w, 1,function(a)sum(a*a))) normmat = do.call(rbind, wlist_rownorms) coverage = sum((sqrt(normmat)&lt; maxdev))/length(normmat) return(coverage) }) -&gt; coverages plot(x = prior_probs, y = coverages, type = &#39;o&#39;, ylim = c(0,1), xlim = c(0,1), xlab = &quot;Target prior probability of deviation\\n of |w| = |X beta| above maxdev&quot;, ylab = &quot;Empirical probability of deviation&quot;) abline(0,1, col = &#39;red&#39;) 3.4 Other helpers Here’s a function to take the stick-breaking \\(\\gamma\\) parameters (with \\(X\\)) and transform them to a probability vector. #&#39; Description goes here #&#39; #&#39; @param ga document this carefully #&#39; @param Xp 1 vector appended to left of covariate matrix X. #&#39; #&#39; @export #&#39; @return (OO x OO) matrix with each row.. post_process_pie &lt;- function(Xp,ga){ Xga &lt;- Xp %*% ga pie.SB &lt;- 1/(1+exp(-Xga)) return(t(apply(pie.SB,1, SB2MN))) } Here’s a helper to change the results from an MCMC to a flowmix- (or flowtrend-) like object. This way, we can use the plotting helpers from those packages. #&#39; Reformatting the results from MCMC to create a flowmix-like object that #&#39; contains of the posterior means of the model parametr. This allows you to use #&#39; plotting code from the flowmix and flowtrend package. #&#39; #&#39; @param res The result of doing MCMC. #&#39; @param last_draws_inds The last few MCMC draws to take. Defaults to NULL. #&#39; #&#39; @return posterior mean estimates of the cluster coefficients, mean, variance #&#39; and probability. #&#39; #&#39; @export #&#39; mcmc_res_to_flowmix &lt;- function(res, last_draws_inds=NULL, last_draws_num = NULL, thin = FALSE){ ## Setup X = t(res$dat.info$X) numclust = res$pos.Sigma %&gt;% .[3] TT = nrow(X) p = ncol(X) numclust = res$dat.info$numclust dimdat = res$dat.info$ylist[[1]] %&gt;% ncol() obj = list() Nmc = res$pos.beta %&gt;% dim() %&gt;% tail(1) if(is.null(last_draws_inds)) last_draws_inds = 1:Nmc if(!is.null(last_draws_inds))stopifnot(all(last_draws_inds&lt;=Nmc)) if(!is.null(last_draws_num)) last_draws_inds = (Nmc-last_draws_num+1):Nmc if(thin){ last_draws_inds = seq(from = (Nmc-last_draws_num+1), to = Nmc, by = 20) } ## last_draw_inds = 1:dim(res$pos.beta)[4] ## Means pos.mn &lt;- list() for(kk in 1:numclust){ pos.mn[[kk]] &lt;- mclapply(last_draws_inds, function(mm){ res$pos.beta[,,kk,mm,drop=TRUE]%*% rbind(1,t(X)) }, mc.cores = detectCores())%&gt;% abind::abind(.,along=3) } post.mn.mean &lt;- lapply(pos.mn, function(aa){ apply(aa,c(1,2),mean)}) obj$mn = array(NA, dim = c(TT, dimdat,numclust)) for(iclust in 1:numclust){ obj$mn[,,iclust] = t(post.mn.mean[[iclust]]) } ## probabilities ## pos.gamma.mean = res$pos.gamma[,,last_draws_inds] %&gt;% apply(c(1:2), mean) pos.SB &lt;- apply(res$pos.gamma[,,last_draws_inds, drop=FALSE], c(2,3), function(ga) 1/(1+exp(-t(ga) %*% rbind(1,t(X))))) pos.MN &lt;- apply(pos.SB, c(1,3), flowcut:::SB2MN) post.pi.mean &lt;- apply(pos.MN,c(1,2), mean) obj$prob = t(post.pi.mean) ## cluster covariances obj$sigma = array(NA, dim = c(numclust, dimdat, dimdat)) obj$numclust = numclust res$pos.Sigma %&gt;% dim() ## post.Sigma.mean &lt;- apply(res$pos.Sigma,c(1,2,3), mean) %&gt;% aperm(c(3,1,2)) pos.Sigma.mean = res$pos.Sigma[,,,last_draws_inds,drop=FALSE] %&gt;% apply(c(1,2,3), mean) %&gt;% aperm(c(3,1,2)) obj$sigma = pos.Sigma.mean ## mean regression coefficients pos.beta = res$pos.beta[,,,last_draws_inds,drop=FALSE] pos.beta.mean = pos.beta %&gt;% apply(c(1,2,3), mean) obj$beta = pos.beta.mean ## prob regression coefficients pos.gamma.mean = res$pos.gamma[,,last_draws_inds,drop=FALSE] %&gt;% apply(c(1:2), mean) obj$alpha = pos.gamma.mean ## Bundle and return obj$numclust = numclust obj$TT = TT class(obj) = &quot;flowmix&quot; return(obj) } Here’s a function to impute the y’s based on a model defined by the posterior mean of the last 500 draws. #&#39; Imputing particles to impute the y&#39;s based on a model defined by the posterior #&#39; mean of the last 500 draws. This is separated out from the Gibbs since the result takes up a lot of memory. #&#39; (This is all code in the Gibbs.fast() function.) #&#39; #&#39; #&#39; @param ylist Original particles #&#39; @param countslist Biomass list #&#39; @param Cbox Bounding box. #&#39; @param mcres The converted MCMC result, using e.g., |mcmc_res_to_flowmix(res, #&#39; last_draws_inds = 1001:1500)| #&#39; @param n.cores number of CPU cores to use #&#39; #&#39; @export impute &lt;- function(ylist, countslist, Cbox, mcres, n.cores=1){ ## if(FALSE){ ## ## Temporary ## ylist = datobj$ylist ## countslist = datobj$countslist ## ## TEMPORARY: Add a bounding box |Cbox| ## bounds.lower &lt;- lapply(datobj$ylist,function(xx) ## Rfast::colMins(xx,value = TRUE)) %&gt;% ## do.call(rbind,.) %&gt;% Rfast::colMins(.,value = TRUE) ## bounds.upper &lt;- lapply(datobj$ylist,function(xx) ## Rfast::colMaxs(xx,value = TRUE)) %&gt;% ## do.call(rbind,.) %&gt;% Rfast::colMaxs(.,value = TRUE) ## Cbox &lt;- cbind(bounds.lower,bounds.upper) ## Cbox &lt;- res$dat.info$Cbox ## datobj$Cbox = Cbox ## ## Temporary ## mcres = mcmc_res_to_flowmix(res, last_draws_inds = 1001:1500) ## } ## Get everything ntlist = sapply(ylist, nrow) TT = length(ylist) NN &lt;- sum(ntlist) dimdat &lt;- ncol(ylist[[1]]) countsTotal &lt;- sapply(countslist,sum) %&gt;% sum() alpha.factor &lt;- NN/countsTotal W.list &lt;- lapply(countslist, function(xx) xx*alpha.factor) ## More processing Censor.list &lt;- lapply(ylist, function(x) censorIndicator(x,Cbox)) censor.01.list &lt;- lapply(Censor.list, function(x) apply(x,1, function(xx){ sum(abs(xx)) &gt; 0})) ## expensive censor.which.list &lt;- lapply(censor.01.list, function(cc) which(cc==TRUE)) censored.ylist &lt;- mapply(function(yy,c01) {yy[c01==TRUE,,drop=FALSE]}, yy = ylist, c01 = censor.01.list) censored.C.list &lt;- mapply(function(c01,cc){cc[c01==TRUE,,drop=FALSE]}, c01 = censor.01.list, cc=Censor.list) censored.W.list &lt;- mapply(function(c01,ww){ww[c01==TRUE]}, c01 = censor.01.list, w=W.list) ntlist.censor &lt;- sapply(censor.01.list, sum) Z.list = flowmix::gate(mcres, ylist, NULL, 1111, eps_estep = 1E-20) censored.Z.list &lt;- mapply(function(c01,zz){zz[c01 == TRUE]}, c01 = censor.01.list, zz = Z.list) ## Sort of expensive samp.region.list &lt;- lapply(1:TT, function(tt){ c01 &lt;- censor.01.list[[tt]] cc &lt;- Censor.list[[tt]] if(sum(c01==TRUE)&gt;1){ apply(cc[c01==TRUE,,drop=FALSE],1,function(xx) ## flowcut:::sample.region(xx,Cbox)) sample.region(xx,Cbox)) }else if(sum(c01==TRUE)==1){ matrix(sample.region(cc[c01==TRUE,,drop=FALSE],Cbox),ncol=1) }else{ NULL } }) ## Get the censored particles Sig.ell = mcres$sigma %&gt;% aperm(c(2,3,1)) imputed.ylist &lt;- parallel::mclapply(1:TT, function(tt) { mu.mat = mcres$mn[tt,,] yy = impute.censored(ww = censored.W.list[[tt]], yy = censored.ylist[[tt]], zz = censored.Z.list[[tt]], cc.info.mat = censored.C.list[[tt]], bounds.mat = samp.region.list[[tt]], ##mu.list[[tt]] mu.mat = mu.mat, Sigma.ell = Sig.ell, dimdat = dimdat) if(dimdat==1 &amp; !is.null(yy)) yy = t(yy) return(yy) }, mc.cores = n.cores, mc.preschedule = FALSE) ## Finally, ca imputed_ylist &lt;- parallel::mcmapply(function(yy, c01, imp){replace(yy, c01, imp)}, yy = ylist, c01 = censor.01.list, imp = imputed.ylist, mc.cores = n.cores, SIMPLIFY = FALSE) return(list(imputed_ylist = imputed_ylist, censor.01.list = censor.01.list, memlist = Z.list)) } "],["syntheticdata.html", "4 Generating synthetic data", " 4 Generating synthetic data Here are some simple functions to generate 1d data. This code uses data from the flowmix AOAS paper. (More generally, the MCMC function takes mainly ylist, countslist, and X as data.) There are two scenarios we will test out in our paper: One-dimensional data, two clusters. Keep censor boundaries, but move the top cluster towards top censor boundary. One-dimensional data, two clusters. Keep means constant, but move the censor boundaries towards the center. (not done yet!) First load existing cytogram data and flowmix model estimates. (The two data files MGL1704-hourly-paper-1d-diam.RDS, 1d-cvres.rds, which can be downloaded from the flowmix R package, is assumed to be in inst/data from the base directory of the repository.) ## dat_1d = readRDS(&quot;~/Downloads/paper-data/MGL1704-hourly-paper-1d-diam.RDS&quot;) # res = readRDS(&quot;~/Downloads/paper-data/1d-cvres.rds&quot;) %&gt;% .$bestres datadir = &quot;../inst/data&quot; dat_1d = readRDS(file.path(datadir, &quot;MGL1704-hourly-paper-1d-diam.RDS&quot;)) res = readRDS(file.path(datadir, &quot;1d-cvres.rds&quot;)) %&gt;% .$bestres ## Take two clusters&#39; model parameters (Picoeukaryotes and Prochlorococcus) orig_model = list() orig_model$alpha = res$alpha[c(3,4),,drop=FALSE] orig_model$beta = res$beta[c(3,4)] orig_model$sigma = res$sigma[c(3,4),1,1,drop=FALSE] orig_model$dimdat = res$dimdat orig_model$numclust = res$numclust orig_model$TT = res$TT ## Covariates are the same orig_model$X = res$X ## Save the &quot;original model&quot; ## saveRDS(orig_model, file=file.path(&quot;~/repos/flowcut/inst/output&quot;, &quot;orig_model.RDS&quot;)) ## outputdir = &quot;~/repos/flowcut/inst/output&quot; outputdir = &quot;../inst/data&quot; saveRDS(orig_model, file=file.path(outputdir, &quot;orig_sim_model.RDS&quot;)) Next, we’ll make several versions of this model with isignal from 0 to 10; (1) isignal=0 means the means are completely overlapping. (2) isignal=10 is the highest signal size (gap between the two means). (TODO: Check object classes and restrict to flowmix or flowcut (but not flowtrend) model objects.) #&#39; From an original set of model parameters (|true_model|), #&#39; generate synthetic 2-cluster 1-dimensional data with equal probabilities. #&#39; #&#39; @param isignal 0 to 10, which generates the means. #&#39; @param orig_model Original model of class flowmix or flowcut; a list that #&#39; contains alpha, beta and TT. #&#39; @param shrink_alpha If TRUE, &quot;shrink&quot; the alpha coefficients to 40% their #&#39; size. Defaults to FALSE. #&#39; @param coef_is_dense If TRUE, add some Gaussian noise on the non-intercept #&#39; coefficients to make them dense (instead of sparse). #&#39; #&#39; @return A list with beta, mn, alpha, prob, X, sigma, TT, numclust. #&#39; @export make_sim_model &lt;- function(orig_model, isignal, shrink_alpha = FALSE, coef_is_dense = FALSE){ ## Setup stopifnot(isignal %in% 0:10) new_model = orig_model new_model$numclust = 2 ## stopifnot(class(orig_model) %in% c(&quot;flowmix&quot;)) ##, &quot;flowcut&quot; ## If dense model is called for, make the coefficients all nonzero if(coef_is_dense){ dense_model = new_model beta1 = dense_model$beta[[1]][-1] beta2 = dense_model$beta[[2]][-1] sd1 = sd(beta1[which(beta1!=0)])/3 sd2 = sd(beta2[which(beta2!=0)])/3 p = nrow(dense_model$beta[[1]]) - 1 set.seed(4891) dense_model$beta[[1]][-1] = dense_model$beta[[1]][-1] + rnorm(n = p, mean=0, sd=sd1) dense_model$beta[[2]][-1] = dense_model$beta[[2]][-1] + rnorm(n = p, mean=0, sd=sd2) alpha = dense_model$alpha[,-1] sd_alpha = sd(alpha[which(alpha!=0)])/3 dense_model$alpha[,-1] = dense_model$alpha[,-1] + rnorm(n=2*p, mean=0, sd = sd_alpha) new_model = dense_model } ## Calculate + renormalize the probabilities link = cbind(1, orig_model$X) %*% t(orig_model$alpha) new_model$prob = exp(link) / rowSums(exp(link)) new_model$prob %&gt;% matplot(type = &#39;l&#39;, lty = 1) ## Take the two intercepts intp_high = orig_model$beta %&gt;% .[[1]]%&gt;% .[&quot;intp&quot;,] intp_low = orig_model$beta %&gt;% .[[2]]%&gt;% .[&quot;intp&quot;,] increment = (intp_high - intp_low)/10 ## Bring the larger mean down. new_model$beta[[1]][&quot;intp&quot;,] = intp_low + increment * isignal new_model$mn = array(NA, dim = c(orig_model$TT, 1, 2)) new_model$mn[,,1] = (cbind(1,new_model$X)) %*% (new_model$beta[[1]]) new_model$mn[,,2] = (cbind(1,new_model$X)) %*% (new_model$beta[[2]]) ## Shrink the probabilities to be closer to each other. if(shrink_alpha){ Xta1 = (new_model$X) %*% ((new_model$alpha[1,-1]) * 0.40) Xta2 = (new_model$X) %*% ((new_model$alpha[2,-1]) * 0.40) ##lines(exp(Xta1)/(exp(Xta1) + exp(Xta2)), ylim = c(0,1)) ##lines(exp(Xta2)/(exp(Xta1) + exp(Xta2)), col = &#39;red&#39;) new_model$prob[,1] = exp(Xta1)/(exp(Xta1) + exp(Xta2)) new_model$prob[,2] = exp(Xta2)/(exp(Xta1) + exp(Xta2)) } return(new_model) } par(mfrow = c(3,1)) new_model = make_sim_model(orig_model, 0) new_model$mn %&gt;% .[,1,] %&gt;% matplot(type=&#39;l&#39;, main = paste0(&quot;isignal=&quot;, 0), ylim = c(-0.5, 0.6)) new_model = make_sim_model(orig_model, 5) new_model$mn %&gt;% .[,1,] %&gt;% matplot(type=&#39;l&#39;, main = paste0(&quot;isignal=&quot;, 5), ylim = c(-0.5, 0.6)) new_model = make_sim_model(orig_model, 10) new_model$mn %&gt;% .[,1,] %&gt;% matplot(type=&#39;l&#39;, main = paste0(&quot;isignal=&quot;, 10), ylim = c(-0.5, 0.6)) Then, we will generate data from this model using the function gen_1d(). #&#39; Generate 1d data with 2 clusters from a list (|true_model|) #&#39; containing true model parameters. #&#39; #&#39; @param true_model List containing beta, alpha, mn, prob, numclust. #&#39; @param nt Particles per time point. #&#39; #&#39; @return Cytograms (a |ylist| object) #&#39; @export gen_1d &lt;- function(true_model, nt = 1000){ ## Setup stopifnot(true_model$numclust == 2) TT = dim(true_model$mn)[1] ## Generate cytograms ylist = list() for(tt in 1:TT){ ## Generate memberships Samples |nt| memberships out of (1:numclust) ## according to the cluster probabilities in |prob|. nt_by_clust = stats::rmultinom(1, size = nt, true_model$prob[tt,]) ## draws = sample(1:numclust, size = nt, replace = TRUE, prob = true_model$prob[tt,]) draws = c(rep(1, nt_by_clust[1]), rep(2, nt_by_clust[2])) y_onetime = list() for(iclust in 1:true_model$numclust){ ntk = nt_by_clust[iclust] membership = rep(iclust, ntk) if(ntk == 0){ y_onetime[[iclust]] = NULL } else { y_onetime[[iclust]] = cbind(MASS::mvrnorm(n = ntk, mu = true_model$mn[tt,,iclust], Sigma = true_model$sigma[iclust,,])) } } y_onetime = purrr::compact(y_onetime) y = do.call(rbind, y_onetime) ## Data ylist[[tt]] = y } return(ylist) } (TODO We’ll generate data particles with probability proportional to 1/biomass.) Testing this function out. ## Generate data set.seed(100) new_model = make_sim_model(orig_model, 8) ylist = gen_1d(new_model, nt = 100) flowtrend::plot_1d(ylist, obj = new_model) ## Censor it ylist = lapply(ylist, function(y){ y = pmin(y, 0.5) }) flowtrend::plot_1d(ylist, obj = new_model) ## Form the censored &quot;box&quot; Cbox = rbind(c(-Inf, 0.5)) (TODO: Maybe we will use fewer than 40 coefficients. Let’s get the top 10 coefficients by importance, and only use them.) "],["gibbs-sampler.html", "5 Gibbs sampler 5.1 The main Gibbs sampler", " 5 Gibbs sampler 5.1 The main Gibbs sampler The main Gibbs sampler is called run.Gibbs.fast(). TODO items: DONE We should write a function that makes a Cbox object. We should write what user.prior refers to. We should write what gg refers to, precisely. &lt;– actually, we should have maxdev as an input. #&#39; Runs a gibbs sampler to sample from the posterior distribution of the flowcut #&#39; model. #&#39; #&#39; @param ylist data. #&#39; @param countslist weights (counts) for data in |ylist|. #&#39; @param X covariates; a (p x T) matrix (TODO: change code so that X is T x p). #&#39; @param numclust NUmber of clusters. #&#39; @param Nmc Number of MCMC iterations #&#39; @param Nburn Number of burn-in iterations. #&#39; @param Cbox Censored box. #&#39; @param user.prior User-supplied prior. Otherwise, prior defaults to ___. #&#39; @param gg Size of Normal prior on the beta parameters. Defaults to NULL, in #&#39; which case the |maxdev| is used to calculate |gg|. See maxdev_to_gg() for #&#39; details. If a value is provided, |maxdev| and |maxdev_prob| are ignored. #&#39; @param maxdev Maximum deviation of the cluster means. #&#39; @param maxdev_prob The probability of a cluster mean deviating from their #&#39; overall mean, over time t=1,.., T. Defaults to 0.99. #&#39; @param verbose Whether to be loud. #&#39; @param warm.start If supplied, restart the MCMC at these values. #&#39; @param n.cores Number of cores for multiple cores. #&#39; @param modified_g_prior Adding a small constant on the diagonal of X^TX to make it #&#39; invertible. #&#39; #&#39; #&#39; @return #&#39; #&#39; @export run.Gibbs.fast &lt;- function(ylist, countslist, X, numclust, Nmc = 3000, Nburn = 500, Cbox = NULL, censorship.info = NULL, user.prior = NULL, gg = NULL, maxdev = NULL, maxdev_prob = 0.99, prior_spec.list = NULL, verbose = FALSE, last.imputed = NULL, last.para = NULL, save.in.prog = FALSE, save.data = TRUE, modified_g_prior = FALSE, n.cores = 1){ ## Basic setup TT &lt;- length(ylist) stopifnot(TT == ncol(X)) p &lt;- dim(X)[1] dimdat = ncol(ylist[[1]]) ntlist = sapply(ylist, nrow) NN &lt;- sum(ntlist) tt.impute &lt;- min(20, floor(Nburn/5)) n.cores = min(n.cores, TT) numclust &lt;- as.integer(numclust) assertthat::assert_that(!is.null(maxdev) | !is.null(gg)) assertthat::assert_that(all(sapply(ylist, nrow)==sapply(countslist, length))) if(is.null(Cbox)){ Cbox &lt;- censorship.info$Cbox } ## Get the mean ball constraint hyperparameter |gg| if(is.null(gg)){ gg &lt;- maxdev_to_gg(t(X), dimdat = 3, maxdev = 0.5, numclust = numclust, prior.prob = maxdev_prob, ggvec = (1:20)/1000, n.cores = n.cores, Nmc = 1e4*2, viz=FALSE, verbose=TRUE) } ## pre computed quantities X.list &lt;- as.list(as.data.frame(X)) Xp &lt;- rbind(1,X) ## p+1 x TT Xp.list &lt;- as.list(as.data.frame(Xp)) countsTotal &lt;- sapply(countslist,sum) %&gt;% sum() alpha.factor &lt;- NN/countsTotal W.list &lt;- lapply(countslist, function(xx) xx*alpha.factor) W.sq.list &lt;- lapply(W.list, function(xx) sqrt(xx)) mt &lt;- lapply(W.list, sum)%&gt;%unlist() MM &lt;- sum(mt) XtXtT &lt;- lapply(X.list,function(x) x%*%t(x)) XtXtTp &lt;- lapply(Xp.list, function(x) x%*%t(x)) ggXtXtTp &lt;- lapply(Xp.list, function(x) x%*%t(x)/gg) XTX &lt;- X%*%t(X) XTXp &lt;- Xp%*%t(Xp) if(modified_g_prior){ eps = 1E-10 XTX = XTX + eps * diag(rep(1,p)) XTXp = XTXp + eps * diag(rep(1,p+1)) } inv.XTX &lt;- Rfast::spdinv(XTX) inv.XTX_gg &lt;- inv.XTX*gg inv.XTXp &lt;- Rfast::spdinv(XTXp) X0 &lt;- rbind(0, X) XTX0 &lt;- X0%*%t(X0) XTX0_gg &lt;- XTX0/gg inv.XTX0_gg &lt;- rbind(0, cbind(0, inv.XTX*gg)) ## Build censored box if(is.null(censorship.info$censored.ylist) &amp; !is.null(Cbox)) { Censor.list &lt;- parallel::mclapply(ylist, function(x) censorIndicator(x,Cbox), mc.cores = n.cores) censor.01.list &lt;- parallel::mclapply(Censor.list, function(x) apply(x,1, function(xx){ sum(abs(xx))&gt;0}), mc.cores = n.cores) censor.which.list &lt;- parallel::mclapply(censor.01.list, function(cc) which(cc==TRUE), mc.cores = n.cores) censored.ylist &lt;- parallel::mcmapply(function(yy,c01) {yy[c01==TRUE,,drop=FALSE]}, yy = ylist, c01 = censor.01.list, mc.cores = n.cores, SIMPLIFY = FALSE) censored.C.list &lt;- parallel::mcmapply(function(c01,cc){cc[c01==TRUE,,drop=FALSE]}, c01 = censor.01.list, cc=Censor.list, mc.cores = n.cores, SIMPLIFY = FALSE) censored.W.list &lt;- parallel::mcmapply(function(c01,ww){ww[c01==TRUE]}, c01 = censor.01.list, w=W.list, mc.cores = n.cores, SIMPLIFY = FALSE) ntlist.censor &lt;- sapply(censor.01.list, sum) nn.censor &lt;- sum(ntlist.censor) if( nn.censor &gt;0){ samp.region.list &lt;- parallel::mclapply(1:TT, function(tt){ c01 &lt;- censor.01.list[[tt]] cc &lt;- Censor.list[[tt]] if(sum(c01==TRUE)&gt;1){ apply(cc[c01==TRUE,,drop=FALSE],1,function(xx) ## flowcut:::sample.region(xx,Cbox)) sample.region(xx,Cbox)) }else if(sum(c01==TRUE)==1){ matrix(sample.region(cc[c01==TRUE,,drop=FALSE],Cbox),ncol=1) }else{ NULL }}, mc.cores = n.cores) }else{ ## Cbox is not binding print(&quot;No active censoring is found. Turn off censor data imputation.&quot;) Cbox &lt;- NULL samp.region.list &lt;- NULL } censorship.info &lt;- list(Cbox = Cbox, samp.region.list = samp.region.list, censor.01.list = censor.01.list, censored.C.list = censored.C.list, censored.W.list = censored.W.list, censored.ylist = censored.ylist) }else{ samp.region.list = censorship.info$samp.region.list censor.01.list = censorship.info$censor.01.list censored.C.list = censorship.info$censored.C.list censored.W.list = censorship.info$censored.W.list censored.ylist = censorship.info$censored.ylist } if(save.data){ dat.info &lt;- list(ylist = ylist, X= X, countslist = countslist, numclust = numclust, Cbox = censorship.info$Cbox) ## store raw data }else{ dat.info &lt;- NULL } ## prior specifications if(is.null(user.prior)){ nu0 = dimdat + 4 ## such that var exists S0 = diag(dimdat)*(nu0-dimdat-1) * (1.5 / qnorm(0.975))^2 a_gamma &lt;- 5 ## IG(a/2, b/2) b_gamma &lt;- (a_gamma - 2 ) * 0.5 ## IG(a/2, b/2)&#39;s prior mean = 0.5 prior.spec.list &lt;- list(nu0 = nu0, S0 = S0, a_gamma = a_gamma, b_gamma = b_gamma, gg = gg) ## inv.Omega &lt;- solve(S1) }else{ nu0 &lt;- user.prior$nu0 S0 &lt;- user.prior$S0 a_gamma = user.prior$ab_gamma[1] b_gamma =user.prior$ab_gamma[2] prior.spec.list &lt;- list(nu0 = nu0, S0 = S0, a_gamma = a_gamma, b_gamma = b_gamma, gg = gg ) } ## initialize if(!is.null(last.para)){ print(&quot;The MCMC continues with a previous draw of model parameters&quot;) beta.ell &lt;- last.para$beta gamma.ell &lt;- last.para$gamma Sig.ell &lt;- last.para$Sigma sig2_gamma &lt;- last.para$sig2_gamma Nburn &lt;- 0 ## no need of burn-in tt.impute &lt;- 0 }else{ if(verbose) print(&quot;The MCMC starts with a draw of model parameters from the prior&quot;) Sig.ell &lt;- matrixsampling::rinvwishart(numclust, nu0, S0 ) ## %&gt;% as.matrix() beta.ell &lt;- array(0, c(dimdat,p+1,numclust)) for(ell in 1:numclust){ beta.ell[,,ell] &lt;- cbind( (2* runif(dimdat)-1)*1e3, rmatnorm.fast(M = matrix(0,nrow = dimdat, ncol=p), U = Sig.ell[,,ell], V = inv.XTX_gg)) } sig2_gamma &lt;- 1/stats::rgamma(1, a_gamma/2, b_gamma/2) gamma.ell &lt;- cbind( (2* runif(dimdat)-1)*1e3, Rfast::matrnorm(numclust-1,p) %*% Rfast::cholesky(inv.XTX) * sqrt(sig2_gamma)) %&gt;% t() } inv_sig_gamma &lt;- 1 / sig2_gamma a_gamma_n &lt;- (numclust-1) * p + a_gamma if(!is.null(last.imputed)){ print(&quot;continue with previously imputed latent variables.&quot;) Z.list &lt;- last.imputed$Z.list ylist &lt;- last.imputed$ylist rm(last.imputed) gc() Nburn &lt;- 0 ## no need of burn-in tt.impute &lt;- 0 } SX.ell &lt;- array(0, c(p,p,numclust)) Sy.ell &lt;- matrix(0,nrow = dimdat, ncol = TT) Sxy.ell &lt;- array(0, c(p,dimdat,numclust)) ## Make empty objects burn.beta &lt;- array(0,c(dimdat,p+1,numclust,Nburn)) burn.Sigma &lt;- array(0,c(dimdat,dimdat,numclust,Nburn)) burn.gamma &lt;- array(0,c(p+1,numclust-1,Nburn)) burn.avgloglik &lt;- rep(NA, Nburn) burn.sig2_gamma &lt;- matrix(NA, nrow = 1, ncol = Nburn) pos.beta &lt;- array(0,c(dimdat,p+1,numclust,Nmc)) pos.Sigma &lt;- array(0,c(dimdat,dimdat,numclust,Nmc)) pos.gamma &lt;- array(0,c(p+1,numclust-1,Nmc)) pos.sig2_gamma &lt;- matrix(NA, nrow = 1, ncol = Nmc) pos.avgloglik &lt;- rep(NA, Nmc) list.iter &lt;- seq(from=Nburn+1, to = Nmc+Nburn, length.out = 6) %&gt;% floor() if(verbose){ start.date &lt;- Sys.time() %&gt;% as.Date() ptm &lt;- NULL print(&quot;The Gibbs sampler starts now.&quot;) print(paste0(&quot;save files to &quot;, getwd())) } ## posterior sampling starts here for(jj in 1:(Nburn+Nmc)) { if(verbose){ if(jj &gt; Nburn){ cat(&quot;MCMC iteration:&quot;, jj-Nburn, fill = TRUE) }else{ cat(&quot;MCMC (burn-in) iteration:&quot;, jj, fill = TRUE) } if(jj ==1 &amp; Nburn &gt; 0){ print(&quot;Burn-in period starts.&quot;) print(Sys.time()) } if(jj == tt.impute + 1){ ptm &lt;- proc.time() print(Sys.time()) } if(jj ==Nburn+1 &amp; Nburn &gt; 0){ plot(burn.avgloglik[(tt.impute+1):Nburn], type=&quot;l&quot;,lwd=2) print(paste(&quot;Burn-in sample size: &quot;, Nburn, sep = &quot; &quot;)) print(&quot;Burn-in period ends.&quot;) print(&quot;Burn-in time cost per iteration, in seconds&quot;) burn.tc &lt;- (proc.time()-ptm)/(Nburn - tt.impute) print(round(burn.tc,2)) print(Sys.time()) print(&quot;Collecting posterior samples......&quot;) print(paste(&quot;Expected time to draw&quot;, Nmc, &quot;posterior samples: &quot;, round(burn.tc[3]*Nmc/60/60,2), &quot; hours&quot;, sep=&quot; &quot;)) } } if(jj == list.iter[1] &amp; save.in.prog){ saveRDS(list(burn.beta, burn.Sigma, burn.gamma, burn.avgloglik, burn.sig2_gamma), file = paste0(&quot;MCMC-numclust&quot;,numclust,&quot;-&quot;,start.date,&quot;-burn.rds&quot;)) }else if(jj %in% list.iter &amp; save.in.prog){ saveRDS(list(pos.beta, pos.Sigma, pos.gamma, pos.avgloglik, pos.sig2_gamma, ast.ylist = ylist, last.Z.list = Z.list), file = paste0(&quot;MCMC-numclust&quot;,numclust,&quot;-&quot;,start.date,&quot;-pos-&quot;, which(list.iter==jj)-1,&quot;.rds&quot;)) } ## expert assignment ### XpGamma &lt;- Rfast::Crossprod(gamma.ell, Xp) ## K-1 x TT pi.sb &lt;- 1/(1+exp(-XpGamma)) ## stick-breaking representation pi.mn &lt;- apply(pi.sb, 2, SB2MN) ## discrete prob vector logpi.list &lt;- parallel::mclapply(1:TT, function(t) log(pi.mn[,t]), mc.cores = min(n.cores,TT)) chol.Sig.ell &lt;- apply(Sig.ell,3, chol) if(dimdat == 1) chol.Sig.ell = rbind(chol.Sig.ell) chol.Sig.list &lt;- lapply(1:numclust,function(kk) matrix(chol.Sig.ell[,kk], nrow = dimdat)) mu.list &lt;- parallel::mclapply(Xp.list, function(xx){ apply(beta.ell, c(1,3), function(bb) bb %*% xx) }, mc.cores= n.cores) logPiZ &lt;- parallel::mcmapply(function(xx, yy, mm, pp){ sapply(1:numclust, function(kk) mvnfast::dmvn(yy, mm[,kk], chol.Sig.list[[kk]], log=TRUE, isChol = TRUE) + pp[kk])}, xx =X.list, yy = ylist, mm = mu.list, pp=logpi.list, mc.cores = n.cores, SIMPLIFY = FALSE) Z.list &lt;- parallel::mclapply(logPiZ, function(pp) apply(pp,1, function(lpi) .Internal(sample(numclust, 1, TRUE, prob = softmax(lpi)))), mc.cores = n.cores) ## censored data imputation if((jj&gt; tt.impute | !is.null(last.para)) &amp; !is.null(Cbox)){ censored.Z.list &lt;- parallel::mcmapply(function(c01,zz){zz[c01==TRUE]}, c01 = censor.01.list, zz=Z.list, mc.cores = n.cores) imputed.ylist &lt;- parallel::mclapply(1:TT, function(tt) { yy = impute.censored(ww = censored.W.list[[tt]], yy = censored.ylist[[tt]], zz = censored.Z.list[[tt]], cc.info.mat = censored.C.list[[tt]], bounds.mat = samp.region.list[[tt]], mu.mat = mu.list[[tt]], Sigma.ell = Sig.ell, dimdat = dimdat) if(dimdat==1 &amp; !is.null(yy)) yy = t(yy) return(yy) }, mc.cores = n.cores, mc.preschedule = FALSE) ylist &lt;- parallel::mcmapply(function(yy, c01, imp){replace(yy, c01, imp)}, yy = ylist, c01 = censor.01.list, imp = imputed.ylist, mc.cores = n.cores, SIMPLIFY = FALSE) ## for(tt in 1:TT){ ## censored_particles = censor.which.list[[tt]] ## ##stopifnot(nrow(imputed.ylist[[tt]]) == length(censored_particles)) ## ylist[[tt]][censored_particles,] &lt;- imputed.ylist[[tt]] ## } } loglik &lt;- loglik_eval(mu.list, chol.Sig.list, W.list, X.list, ylist, Z.list, as.list(ntlist), simple = TRUE) ## print(sort(round(n.ell/NN,3))) ## print(sort(round(m.ell/MM,3))) if(jj %% 10 == 0 &amp; verbose) print(paste(&quot;avg loglikelihood: &quot;, round(loglik,2), sep=&quot; &quot;)) ## SBMN-logit parameter estimation mt.ell &lt;- parallel::mcmapply(function(ww,zz){ sapply(1:numclust, function(kk) sum(ww[zz==kk]))}, ww = W.list, zz = Z.list, SIMPLIFY = TRUE, mc.cores = n.cores) m.ell &lt;- Rfast::rowsums(mt.ell) XpGamma.abs &lt;- Rfast::Crossprod(gamma.ell, Xp) %&gt;% abs() ## numclust-1 x TT mt.cumsum &lt;- Rfast::colCumSums(as.matrix(mt.ell)) Mt.ell &lt;- rbind(mt, -sweep(mt.cumsum[-numclust,,drop=FALSE], 2, mt.cumsum[numclust, , drop=FALSE])) omega.tell &lt;- matrix(parallel::mcmapply(pgdraw.mod, round(Mt.ell[-numclust,]), XpGamma.abs, mc.cores = n.cores), nrow=numclust-1, ncol = TT) kappa &lt;- mt.ell[-numclust, , drop=FALSE] - Mt.ell[-numclust, , drop=FALSE]/2 ## if(min(m.ell)&lt;1) browser() numclust.pos &lt;- which(m.ell&gt;0) %&gt;% max() if(numclust.pos &lt; (numclust - 1)){ V.omell &lt;- parallel::mclapply(1 : numclust.pos, function(ell){ Rfast::spdinv(Reduce(&#39;+&#39;, Map(&#39;*&#39;, XtXtTp, omega.tell[ell,])) + XTX0 * inv_sig_gamma)}, mc.cores = n.cores) gamma.ell[ , 1 : numclust.pos] &lt;- parallel::mclapply(1 : numclust.pos, function(ell){ Rfast::rmvnorm(1, V.omell[[ell]] %*% Reduce(&#39;+&#39;, Map(&#39;*&#39;, Xp.list , kappa[ell,])), V.omell[[ell]])} , mc.cores = n.cores) %&gt;% do.call(rbind, .) %&gt;% t() ## empty cluster, sample from the prior gamma.ell[1, (numclust.pos+1) : (numclust-1)] &lt;- (runif(numclust - 1 - numclust.pos)-0.5)*10 gamma.ell[2:(p+1), (numclust.pos+1):(numclust-1)] &lt;- t(Rfast::matrnorm(numclust-1 - numclust.pos , p ) %*% Rfast::cholesky(inv.XTX)) * sqrt(sig2_gamma) }else{ V.omell &lt;- parallel::mclapply(1 : (numclust-1), function(ell){ Rfast::spdinv(Reduce(&#39;+&#39;, Map(&#39;*&#39;, XtXtTp, omega.tell[ell,])) + XTX0 * inv_sig_gamma)}, mc.cores = n.cores) gamma.ell &lt;- parallel::mclapply(1 : (numclust-1), function(ell){ Rfast::rmvnorm(1, V.omell[[ell]] %*% Reduce(&#39;+&#39;, Map(&#39;*&#39;, Xp.list , kappa[ell,])), V.omell[[ell]])} , mc.cores = n.cores) %&gt;% do.call(rbind, .) %&gt;% t() } b_gamma_n &lt;- apply(t(X) %*% gamma.ell[-1, ], 2, crossprod) %&gt;% sum() + b_gamma inv_sig_gamma &lt;- stats::rgamma(1, a_gamma_n/2, b_gamma_n/2) sig2_gamma &lt;- 1 / inv_sig_gamma ## a_gamma_n &lt;- p + a_gamma ## b_gamma_n &lt;- apply(t(X) %*% gamma.ell[-1, ], 2, crossprod) + b_gamma ## inv_sig_gamma &lt;- stats::rgamma(numclust - 1, a_gamma_n/2, b_gamma_n/2) ## sig2_gamma &lt;- 1 / inv_sig_gamma ## experts&#39; estimation for(ell in 1:numclust){ ## sample beta (including the intercept) jointly if(m.ell[ell]&gt;0){ SX.ell &lt;- XTX0_gg + Reduce(&#39;+&#39;, Map(`*`, XtXtTp, mt.ell[ell,])) inv.SX.ell &lt;- Rfast::spdinv(SX.ell) ## (p+1) x (p+1) Sy.ell &lt;- parallel::mcmapply(function(yy,zz,ww){ Rfast::Crossprod(as.matrix(ww[zz==ell]), yy[zz==ell,,drop=FALSE])}, ww = W.list, yy = ylist, zz = Z.list, SIMPLIFY = TRUE, mc.cores = n.cores) if(dimdat == 1){ Sxy.ell &lt;- Xp %*% as.matrix(Sy.ell) ## (p+1) x 1 } else { Sxy.ell &lt;- Rfast::Tcrossprod(Xp, Sy.ell) ## (p+1) x d } beta.ell[,,ell] &lt;- rmatnorm.fast(M = crossprod(Sxy.ell,inv.SX.ell), U = Sig.ell[,,ell], V = inv.SX.ell) sse &lt;- parallel::mcmapply(function(ww,xx,yy,zz) { wcrossprod.fast(Rfast::eachrow(as.matrix(yy[zz==ell,,drop=FALSE]), beta.ell[,,ell]%*%xx, &#39;-&#39;), ww[zz==ell], weighting = TRUE)}, xx = Xp.list, yy = ylist, zz=Z.list, ww = W.sq.list, SIMPLIFY = FALSE, mc.cores = n.cores) if(dimdat == 1) Sn.ell = sum(unlist(sse)) if(dimdat &gt; 1) { Sn.ell &lt;- Reduce(&#39;+&#39;, sse[sapply(sse,length)&gt;0] ) if(! isSymmetric(Sn.ell)){ ## check symmetry Sn.ell &lt;- (Sn.ell+t(Sn.ell))/2 } } Sig.ell[,,ell] &lt;- matrixsampling::rinvwishart(1, nu0 + m.ell[ell], S0 + Sn.ell , checkSymmetry=FALSE )[,,1] }else{ ## browser() ## no particles allocated, empty class-&gt; no data, sample from the prior beta.ell[,,ell] &lt;- cbind((2* runif(dimdat)-1)*1e3, rmatnorm.fast(M = matrix(0,nrow = dimdat, ncol=p), U = Sig.ell[,,ell], V = inv.XTX_gg) ) Sig.ell[,,ell] &lt;- matrixsampling::rinvwishart(1,nu0,S0)[,,1] } } ## collecting posterior samples if(jj &gt; Nburn){ pos.beta[,,,jj-Nburn] &lt;- beta.ell pos.Sigma[,,,jj-Nburn] &lt;- Sig.ell pos.gamma[,,jj-Nburn] &lt;- gamma.ell pos.sig2_gamma[,jj-Nburn] &lt;- sig2_gamma pos.avgloglik[jj-Nburn] &lt;- loglik } else{ burn.beta[,,,jj] &lt;- beta.ell burn.Sigma[,,,jj] &lt;- Sig.ell burn.gamma[,,jj] &lt;- gamma.ell burn.sig2_gamma[,jj] &lt;- sig2_gamma burn.avgloglik[jj] &lt;- loglik } } ## MCMC stops here. if(verbose){ print(&quot;All work is done.&quot;) print(paste(&quot;Stored MCMC sample size: &quot;, Nmc, sep = &quot; &quot;)) print(Sys.time()) print(&quot;total time cost, in min&quot;) print(round((proc.time()-ptm)/60,1)) print(&quot;total time cost, in hours&quot;) print(round((proc.time()-ptm)/60/60,2)) print(&quot;time cost per iteration, in seconds&quot;) print((proc.time()-ptm)/(Nburn+Nmc - tt.impute)) } last.imputed &lt;- list(last.ylist = ylist, last.Z.list = Z.list) last.para &lt;- list(last.gamma = gamma.ell, last.beta = beta.ell, last.Sigma = Sig.ell, last.sig2_gamma = sig2_gamma) if(is.null(Cbox)){ pos.imputed.Y = NULL censorship.info &lt;- NULL } else { pos.imputed.Y = do.call(rbind, imputed.ylist) } ret &lt;- list( burn.beta = burn.beta, burn.Sigma = burn.Sigma, burn.gamma = burn.gamma, burn.sig2_gamma = burn.sig2_gamma, pos.beta = pos.beta, pos.Sigma = pos.Sigma, pos.gamma = pos.gamma, pos.imputed.Y = pos.imputed.Y , pos.sig2_gamma = pos.sig2_gamma, burn.avgloglik = burn.avgloglik, pos.avgloglik = pos.avgloglik, dat.info = dat.info , last.imputed = last.imputed, last.para = last.para, prior.spec = prior.spec.list, censorship.info = censorship.info) return(ret) } "],["simulations.html", "6 Simulations 6.1 Simulation helpers", " 6 Simulations Refer back to the data generation in @ref{syntheticdata}. We will be top-censoring the data at 0.5. Before censoring, this is what the data looks like. ## Load the &quot;original&quot; model orig_model = readRDS(file=file.path(&quot;~/repos/flowcut/inst/output&quot;, &quot;orig_model.RDS&quot;)) ## Generate data set.seed(100) isignal = 10 new_model = flowcut::make_model(orig_model, isignal) ## new_model = flowcut::make_model(orig_sim_model, isignal) ylist = flowcut::gen_1d(new_model, nt = 100) flowtrend::plot_1d(ylist, obj = new_model) After censoring, this is it. ## Censor it ylist = lapply(ylist, function(y){ y = pmin(y, 0.5) }) flowtrend::plot_1d(ylist, obj = new_model) We need to specify a few things (1) like the censoring limits Cbox and (2) countslist before running the MCMC. ## Form the censoring &quot;box&quot; Cbox = rbind(c(-Inf, 0.5)) ## Counts are all equal for now countslist = lapply(ylist, function(y){ rep(1, nrow(y)) }) ## Save the metadata datobj = list(ylist=ylist, countslist=countslist, Cbox=Cbox, X=orig_model$X) ## datobj = list(ylist=ylist, countslist=countslist, Cbox=Cbox, X=orig_sim_model$X) saveRDS(datobj, file.path(&quot;~/repos/flowcut/inst/output&quot;, paste0(&quot;isignal-&quot;, isignal, &quot;-datobj.RDS&quot;))) We also need some prior elicitation to prevent the cluster means from changing too much across time. (code copy-pasted as-is for now, not meant to be run) gg &lt;- maxdev_to_gg(datobj$X, dimdat = 3, maxdev = 0.5, numclust = 10, ggvec = (1:40)/200, n.cores = detectCores(), Nmc = 1e4, viz=FALSE) gg &lt;- 0.01287879 ## hist(ball.deviance(0.01,0.5,t(X))$msd, breaks = &quot;FD&quot;) ## hist(ball.deviance(0.1,0.5,X)$msd, breaks = &quot;FD&quot;) ## hist(ball.deviance(0.2,0.5,X)$msd, breaks = &quot;FD&quot;) ## hist(ball.deviance(0.5,0.5,X)$msd, breaks = &quot;FD&quot;) ## hist(ball.deviance(1,0.5,X)$msd, breaks = &quot;FD&quot;) ## hist(ball.deviance(2,0.5,X)$msd, breaks = &quot;FD&quot;) Next, we run the MCMC. ## Run the MCMC Nmc &lt;- 1e3 * 5 Nburn &lt;- 500 Nmc &lt;- 20 Nburn &lt;- 10 set.seed(123) Gibbs.res &lt;- run.Gibbs.fast(ylist = datobj$ylist, countslist = datobj$countslist, numclust = 2, Nmc = Nmc, Nburn = Nburn, gg = 0.1, X = t(datobj$X), Cbox = datobj$Cbox, verbose = TRUE) ## Save the results saveRDS(Gibbs.res, file.path(&quot;~/repos/flowcut/inst/output&quot;, paste0(&quot;isignal-&quot;, isignal, &quot;-gibbs.RDS&quot;))) 6.1 Simulation helpers Here is a helper function for obtaining the membership probabilities of a new dataset |ynew| given GMM model |gmm_model|. #&#39; Get the membership probabilities of a new dataset |ynew| given GMM model |gmm_model|. #&#39; #&#39; @param gmm_model from mclust or me.weighted #&#39; @param ynew new data to get predicted posterior membership for. #&#39; #&#39; @return estimated posterior probabilities from an estimated GMM model get_posterior_probs &lt;- function(gmm_model, ynew){ stopifnot(ncol(ynew) == 1) numclust = length(gmm_model$parameters$mean) mn = gmm_model$parameters$mean sd = gmm_model$parameters$variance$sigmasq %&gt;% sqrt() pro = gmm_model$parameters$pro post_prob = lapply(1:numclust, function(iclust){ weighted_d1 = dnorm(ynew, mean = mn[iclust], sd = sd[iclust]) * pro[iclust] }) %&gt;% do.call(cbind, .) post_prob = post_prob/rowSums(post_prob) ## Give up on posterior probability calculation and make all things 1/2, when ## this (rarely) happens. if(any(is.nan(post_prob))) post_prob = matrix(1/2, nrow=nrow(post_prob), ncol=2) return(post_prob) } Also, here is a helper to split the data into (1) censored and (2) non-censored (“inner”) particles; used in preimpute(). #&#39; A helper to split the data into (1) censored and (2) non-censored (&quot;inner&quot;) particles. #&#39; #&#39; @param datobj List containing ylist, countslist, X, and Cbox. #&#39; #&#39; @return List of particles on /and/ inside censor boundary. And the directions #&#39; in which they were censored. #&#39; #&#39; @export split_data &lt;- function(datobj){ ## At each time point, set aside censored points. censor.direction.full.list &lt;- lapply(datobj$ylist, function(x){ flowcut:::censorIndicator(x[,1, drop=FALSE], datobj$Cbox[1,,drop=FALSE])}) censor.which.list = censor.direction.full.list %&gt;% lapply(function(x) apply(x,1, function(xx){ sum(abs(xx)) &gt; 0 })) %&gt;% lapply(which) censored_ylist &lt;- mapply(function(y, censor_which){y[censor_which,,drop=FALSE]}, datobj$ylist, censor.which.list, SIMPLIFY = FALSE) censored_direction &lt;- mapply(function(censor_direction, censor_which){ censor_direction[censor_which,,drop=FALSE] }, censor.direction.full.list, censor.which.list, SIMPLIFY = FALSE) ## here are data inner_ylist &lt;- mapply(function(y, censor_which){ if(length(censor_which) &gt; 0) return(y[-censor_which,,drop=FALSE]) if(!length(censor_which) &gt; 0) return(y) }, datobj$ylist, censor.which.list, SIMPLIFY = FALSE) inner_countslist &lt;- mapply(function(counts, censor_which){ if(length(censor_which) &gt; 0) return(counts[-censor_which]) if(!length(censor_which) &gt; 0) return(counts) }, datobj$countslist, censor.which.list, SIMPLIFY = FALSE) return(list(censored_ylist = censored_ylist, censored_direction = censored_direction, inner_ylist = inner_ylist, inner_countslist = inner_countslist, censor.which.list = censor.which.list )) } Here is a function that will pre-impute data for a 2-cluster 1-dimensional dataset; used in preimpute(). #&#39; At each time point, we will (1) set aside the censored points, and (2) estimate #&#39; a 2-component GMM (2) take the censored points, and &quot;impute&quot; them according to those #&#39; mixtures. (After that, we&#39;ll (3) fit flowmix on the resulting imputed data.) #&#39; #&#39; This is only designed for 1-dimensional data with two clusters. #&#39; #&#39; @param datobj List with ylist (particles), countslist, X, and Cbox. #&#39; #&#39; @return datobj with imputed ylist. #&#39; #&#39; @export preimpute &lt;- function(datobj){ ## Setup stopifnot(ncol(datobj$ylist[[1]]) == 1) stopifnot(&quot;Cbox&quot; %in% names(datobj)) ## Split the data splitres = split_data(datobj) inner_ylist = splitres$inner_ylist inner_countslist = splitres$inner_countslist censored_ylist = splitres$censored_ylist censored_direction = splitres$censored_direction censor.which.list = splitres$censor.which.list ## What happens when the &quot;inner&quot; data is empty-ish at some times? We sample ## some points from the rest of the data, and fill the empty-ish cytograms in ## so that there are at least 5 particles in each cytogram. inner_ntlist = sapply(inner_ylist, nrow) min_num_particles = 5 times_empty = which(inner_ntlist &lt; min_num_particles) collapsed_ylist = inner_ylist[-times_empty] %&gt;% lapply(function(y){sample(y, size=ceiling(nrow(y)/3), replace=FALSE)}) %&gt;% Reduce(c, .) if(length(times_empty) &gt; 0){ for(tt_empty in times_empty){ nt = length(inner_ylist[[tt_empty]]) extra_points = sample(collapsed_ylist, min_num_particles - nt) %&gt;% cbind() inner_ylist[[tt_empty]] &lt;- rbind(inner_ylist[[tt_empty]], extra_points) inner_countslist[[tt_empty]] &lt;- c(inner_countslist[[tt_empty]], rep(1, length(extra_points))) } } stopifnot(all(sapply(inner_ylist, nrow)&gt;=min_num_particles)) ## Fit 2-mixture Gaussian at each time point. gmm_model_list &lt;- mapply(function(y, counts){ ## z is just a random initialization of the memberships of the points. z = mclust::unmap(sample(1:2, size = length(y), replace=TRUE)) ## Fit a Gaussian mixture model for weighted data. ##res = mclust::me.weighted(as.numeric(y), modelName = &quot;V&quot;, weights = counts, z = z) res = mclust::Mclust(as.numeric(y), G=2, modelName = &quot;V&quot;, verbose = FALSE, warn = FALSE)##, weights = counts, z = z) if(is.null(res)){ res = mclust::Mclust(as.numeric(y), G=2, modelName = &quot;E&quot;, verbose = FALSE, warn = FALSE)##, weights = counts, z = z) res$parameters$variance$sigmasq = rep(res$parameters$variance$sigmasq,2) } assertthat::assert_that(!is.null(res)) return(res) }, inner_ylist, inner_countslist, SIMPLIFY = FALSE) ## Impute the points imputed_censored_ylist = mapply(function(gmm_model, ynew, dirs){ ## Impute based on 2-cluster model if(nrow(ynew) == 0) return(ynew) ynew_imputed = ynew post_probs = get_posterior_probs(gmm_model, ynew) new_mems = post_probs %&gt;% apply(1, function(ps){ sample(1:2, 1, prob=ps) }) ## Get the gaussian means and SDs means = gmm_model$parameters$mean sds = gmm_model$parameters$variance$sigmasq %&gt;% sqrt() ## Information for imputing each points impute_mat = tibble(new_mems = new_mems, dir = dirs) ## Impute the points for(irow in 1:nrow(impute_mat)){ iclust = impute_mat$new_mems[irow] if(impute_mat$dir[irow] == 1){ imputed_point = truncnorm::rtruncnorm(1, a = datobj$Cbox[1,2], mean = means[iclust], sd = sds[iclust]) } if(impute_mat$dir[irow] == -1){ imputed_point = truncnorm::rtruncnorm(1, b = datobj$Cbox[1,1], mean = means[iclust], sd = sds[iclust]) } ynew_imputed[irow] = imputed_point } return(ynew_imputed) }, gmm_model_list, censored_ylist, censored_direction, SIMPLIFY = FALSE) stopifnot(all(sapply(imputed_censored_ylist, nrow) == sapply(censored_ylist, nrow))) ## Return the datobj (ylist, countslist, and flowmix). TT = length(datobj$ylist) new_ylist = datobj$ylist for(tt in 1:TT){ inds = censor.which.list[[tt]] if(length(inds) == 0) next ynew = datobj$ylist[[tt]] ynew[inds] = imputed_censored_ylist[[tt]] new_ylist[[tt]] = ynew } stopifnot(all(sapply(new_ylist, nrow) == sapply(datobj$ylist, nrow))) new_datobj = datobj new_datobj$ylist = new_ylist return(new_datobj) } "],["real-data-example.html", "7 Real data example", " 7 Real data example We run the Gibbs sampler on real data. We’ll load the data, process it, then feed it into the MCMC. TODO: bundle the Cbox into a make_Cbox() function. Why are we doing more.censor() manually? Simplify/streamline this. ## Load the code for running MCMC ## codedir = &quot;~/Dropbox/Apps/Overleaf/censor-flowmix/BMEcensor/code/&quot; codedir = &quot;~/Dropbox/Apps/Overleaf/censor-flowmix/BMEcensor/code/&quot; source(file.path(codedir, &#39;BME-functions.r&#39;)) ## Load dataset datadir = &quot;~/Dropbox/research-new/censored-flowmix/code/data&quot; dta &lt;- readRDS(file.path(datadir, &quot;pre-censor-datobj.RDS&quot;)) ## Assign data to objects ylist &lt;- dta$ylist TT &lt;- length(ylist) d &lt;- 3 countslist = dta$countslist ## Define censor limits bounds.lower &lt;- rowMins(matrix(unlist( lapply(dta$ylist, function(xx) colMins(xx,value = TRUE))), nrow = d), value = TRUE) bounds.upper &lt;- rowMaxs(matrix(unlist( lapply(dta$ylist, function(xx) colMaxs(xx,value = TRUE))), nrow = d), value = TRUE) ## Censor the first dimension a bit more Cbox &lt;- cbind(signif(log(bounds.lower),5), log(bounds.upper)) more.censor &lt;- function(ymat,bd){ cbind(sapply(ymat[,1], function(xx) ifelse(xx&lt;bd, bd, xx)), ymat[,c(2,3)]) } ylist &lt;- mclapply(dta$ylist, function(yy){ more.censor(log(yy),Cbox[1,1]), }, mc.cores = min(n.cores, TT)) ## Checking that censoring is done properly assertthat::assert_that(all(rowMins(matrix(unlist( lapply(ylist, function(xx) colMins(xx,value = TRUE))), nrow = d), value = TRUE) == Cbox[,1])) assertthat::assert_that(all(rowMaxs(matrix(unlist( lapply(ylist, function(xx) colMaxs(xx,value = TRUE))), nrow = d), value = TRUE) == Cbox[,2])) ## Clean data datobj &lt;- list(ylist = ylist, countslist = dta$countslist, X = dta$X, numclust = 10, Cbox=Cbox) ## Run the MCMC Nmc &lt;- 1e3*5 Nburn &lt;- 500 K &lt;- 10 Gibbs.res0 &lt;- run.Gibbs.fast(ylist = datobj$ylist, countslist = datobj$countslist, X = datobj$X, numclust = datobj$numclust, gg = 0.1, Nmc = Nmc, Nburn = Nburn, Cbox = datobj$Cbox, verbose = TRUE, warm.start = NULL) "],["documenting-the-package-and-building.html", "8 Documenting the package and building", " 8 Documenting the package and building We finish by running commands that will document, build, and install the package. It may also be a good idea to check the package from within this file. litr::document() # &lt;-- use instead of devtools::document() ## ℹ Updating flowcut documentation ## Writing &#39;NAMESPACE&#39; ## ℹ Loading flowcut ## Loading required package: MASS ## ## ## Attaching package: &#39;MASS&#39; ## ## ## The following object is masked from &#39;package:dplyr&#39;: ## ## select ## ## ## Loading required package: matrixsampling ## ## Loading required package: mvnfast ## ## ## Attaching package: &#39;mvnfast&#39; ## ## ## The following object is masked from &#39;package:lubridate&#39;: ## ## ms ## ## ## Loading required package: parallel ## ## Loading required package: pgdraw ## ## Loading required package: Rfast ## Warning: package &#39;Rfast&#39; was built under R version 4.4.1 ## Loading required package: Rcpp ## Warning: package &#39;Rcpp&#39; was built under R version 4.4.1 ## Loading required package: RcppZiggurat ## Loading required package: RcppParallel ## Warning: package &#39;RcppParallel&#39; was built under R version 4.4.1 ## ## Attaching package: &#39;RcppParallel&#39; ## ## The following object is masked from &#39;package:Rcpp&#39;: ## ## LdFlags ## ## ## Rfast: 2.1.4 ## ___ __ __ __ __ __ __ __ __ __ _ _ __ __ __ __ __ __ __ __ __ __ __ ## | __ __ __ __ | | __ __ __ __ _/ / \\ | __ __ __ __ / /__ __ _ _ __ __\\ ## | | | | | | / _ \\ | | / / ## | | | | | | / / \\ \\ | | / / ## | | | | | | / / \\ \\ | | / / ## | |__ __ __ __| | | |__ __ __ __ / / \\ \\ | |__ __ __ __ _ / /__/\\ ## | __ __ __ __| | __ __ __ __| / /__ _ __\\ \\ |_ __ __ __ _ | / ___ / ## | \\ | | / _ _ _ _ _ _ \\ | | \\/ / / ## | |\\ \\ | | / / \\ \\ | | / / ## | | \\ \\ | | / / \\ \\ | | / / ## | | \\ \\ | | / / \\ \\ | | / / ## | | \\ \\__ __ _ | | / / \\ \\ _ __ __ __ _| | / / ## |_| \\__ __ __\\ |_| /_/ \\_\\ /_ __ __ __ ___| \\/ team ## ## Attaching package: &#39;Rfast&#39; ## ## The following objects are masked from &#39;package:mvnfast&#39;: ## ## dmvt, rmvt ## ## The following object is masked from &#39;package:dplyr&#39;: ## ## nth ## ## The following objects are masked from &#39;package:purrr&#39;: ## ## is_integer, transpose ## ## Loading required package: tmvnsim ## ✖ run.Gibbs.fast.R:27: @return requires a value.Writing &#39;NAMESPACE&#39;Writing &#39;printprogress.Rd&#39;Writing &#39;progress.Rd&#39;Writing &#39;SB2MN.Rd&#39;Writing &#39;MN2SB.Rd&#39;Writing &#39;flowcut-package.Rd&#39;Writing &#39;gen_1d.Rd&#39;Writing &#39;get_Cbox.Rd&#39;Writing &#39;get_posterior_probs.Rd&#39;Writing &#39;impute.Rd&#39;Writing &#39;make_sim_model.Rd&#39;Writing &#39;maxdev_to_g_closed_form_1d.Rd&#39;Writing &#39;maxdev_to_g_closed_form_3d.Rd&#39;Writing &#39;maxdev_to_gg.Rd&#39;Writing &#39;mcmc_res_to_flowmix.Rd&#39;Writing &#39;post_process_pie.Rd&#39;Writing &#39;preimpute.Rd&#39;Writing &#39;run.Gibbs.fast.Rd&#39;Writing &#39;split_data.Rd&#39;Writing &#39;pipe.Rd&#39; "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
